{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcH5lEUWjdr-",
        "outputId": "ac218f6e-071d-441e-8a45-8d5d150d77aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "IYc1D_tkldcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_size=\"gpt2\"):\n",
        "    \"\"\"\n",
        "    Load GPT-2 model and tokenizer.\n",
        "    Available model sizes: \"gpt2\" (small), \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
        "    \"\"\"\n",
        "    print(f\"Loading {model_size} model...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_size)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_size)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "NjHEZB4olttJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, prompt, max_length=100,\n",
        "                  temperature=1.0, top_k=50, top_p=0.95, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generate text using GPT-2 model.\n",
        "\n",
        "    Args:\n",
        "        model: The GPT-2 model\n",
        "        tokenizer: The GPT-2 tokenizer\n",
        "        prompt: Input text to start generation\n",
        "        max_length: Maximum length of generated text\n",
        "        temperature: Controls randomness (lower = more deterministic)\n",
        "        top_k: Number of highest probability tokens to consider\n",
        "        top_p: Cumulative probability threshold for nucleus sampling\n",
        "        num_return_sequences: Number of different sequences to generate\n",
        "\n",
        "    Returns:\n",
        "        Generated text\n",
        "    \"\"\"\n",
        "    # Encode the input text\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_texts = []\n",
        "    for i, output in enumerate(outputs):\n",
        "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        generated_texts.append(generated_text)\n",
        "\n",
        "    return generated_texts"
      ],
      "metadata": {
        "id": "elJfwSF-l05Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = load_model(\"gpt2\")  # Use \"gpt2\" for faster loading\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Once upon a time in a village named vijaypur\"\n",
        "\n",
        "# Generate text\n",
        "generated_texts = generate_text(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    prompt=prompt,\n",
        "    max_length=200,  # Total length including prompt\n",
        "    temperature=0.8,  # Lower for more focused output, higher for more creative\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3  # Generate 3 different outputs\n",
        ")\n",
        "\n",
        "# Print the generated text\n",
        "for i, text in enumerate(generated_texts):\n",
        "    print(f\"\\n--- Generated Text {i+1} ---\")\n",
        "    print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWeTq1XlmHAQ",
        "outputId": "bfbefb98-2ccc-430f-fd7f-ff56629b5602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading gpt2 model...\n",
            "\n",
            "--- Generated Text 1 ---\n",
            "Once upon a time in a village named vijaypur, the leader of a small clan of the Aryan race, called the Akhkar, came into the village. As he was leaving the village, he noticed that there was a strange smell in the air. As he walked out of the village, he saw the Akhkar. He saw a black cloaked figure and suddenly, the Akhkar's eyes began to turn to water. The Akhkar was about to kill him when he opened his mouth and saw the Akhkar's eyes.\n",
            "\n",
            "It was a scene like nothing his ancestors ever experienced. The Akhkar's eyes glowed and lit up, and it turned out that he had become a water-based creature. He started to attack the Akhkar, and when he got close enough to the Akhkar's eyes, he turned into water. As he turned into water, he saw that the Akhkar's eyes turned to red.\n",
            "\n",
            "--- Generated Text 2 ---\n",
            "Once upon a time in a village named vijaypur, a girl named Sushil Kumar, a village doctor, was kidnapped. She was brought to a house in the village by her mother. Her father had sent her to a jail, where she was tortured. She was brought to her father's house and forced to do some labor. A man named Srinivasan, a doctor, was sent to her house. She was told to perform some tasks. He told her that he was going to take her to another village, where he said, \"I am going to rape you. I am going to kill you.\" Srinivasan was arrested, and he had a gun pointed at her head. She was told that her husband was going to kill her if she did not confess to her actions. Then, when she was questioned about the rape, he confessed that he had taken her to an Indian hospital, where he told her that she was raped because she was a woman\n",
            "\n",
            "--- Generated Text 3 ---\n",
            "Once upon a time in a village named vijaypur, he found himself stuck in the sand in a ditch. He found himself being attacked by a group of bandits. He was shot by one of the bandits who was trying to kill him. After killing him, he had to return to his village to save his life.\n",
            "\n",
            "The next day, he was again attacked by bandits. He heard a scream. The bandits used a weapon and started attacking him. He was shot by one of the bandits. He was killed by another group of bandits and two of them had to be saved by a group of villagers.\n",
            "\n",
            "Afterwards, he went back to his village and killed the bandits who were killing him.\n",
            "\n",
            "In the next two days, he found that he was going to find a way to escape. He managed to kill the bandits who were attacking him. The bandits then attacked him again. The bandits killed him.\n"
          ]
        }
      ]
    }
  ]
}